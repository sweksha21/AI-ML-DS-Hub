# -*- coding: utf-8 -*-
"""email spam classification

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZyCSesWQihXZ3hK39CjCUWrHCaD55RRX
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import re
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize


# Ensure you have NLTK stopwords downloaded
import nltk
nltk.download('stopwords')
nltk.download('punkt')

#load data
file_path = 'combined_data.csv'
data = pd.read_csv(file_path)

data.head(5)

plt.figure(figsize=(18,9))
sns.countplot(x='label',data=data, hue=data.label)
plt.title('distribution of labels')
plt.xlabel('label')
plt.ylabel('count')
plt.show()

# Distribution of Labels as a Pie Chart
plt.figure(figsize=(10, 6))
label_counts = data['label'].value_counts()
plt.pie(label_counts, labels=label_counts.index, autopct='%1.1f%%', startangle=140)
plt.title('Distribution of Labels')
plt.axis('equal')  # Equal aspect ratio ensures the pie is drawn as a circle.
plt.show()

# Calculate text length
data['text_length'] = data['text'].apply(len)

# Distribution of Text Length
plt.figure(figsize=(10, 6))
sns.histplot(data['text_length'], bins=8000, kde=False)
plt.title('Distribution of Text Length')
plt.xlabel('Text Length')
plt.ylabel('Frequency')
plt.xlim(0,3000)
plt.show()

# Function to remove hyperlinks
def remove_hyperlinks(text):
    return re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)

# Function to convert text to lowercase
def text_to_lower(text):
    return text.lower()

# Function to remove numbers
def remove_numbers(text):
    return re.sub(r'\d+', ' ', text)

# Function to remove punctuation
def remove_punctuation(text):
    return re.sub(f'[{string.punctuation}]', ' ', text)

# Function to remove extra whitespaces
def remove_whitespaces(text):
    text = text.strip()
    return re.sub('\s+', ' ', text)

# Function to remove newlines
def remove_newlines(text):
    return text.replace('\n', ' ').replace('\r', '')

# Final preprocessing function that calls all the above functions sequentially
def preprocess_text(text):
    text = remove_hyperlinks(text)
    text = text_to_lower(text)
    text = remove_numbers(text)
    text = remove_punctuation(text)
    text = remove_whitespaces(text)
    text = remove_newlines(text)
    return text

from sklearn.model_selection import train_test_split

train_email, test_email, train_label, test_label = train_test_split(data.text, data.label, test_size = 0.2)

train_email, train_label, len(train_email), len(train_label)

x_train = [preprocess_text(sentence) for sentence in train_email]
x_test =  [preprocess_text(sentence) for sentence in test_email]


x_train, x_test, len(x_train), len(x_test)

y_train = train_label
y_test = test_label

y_train, y_test, len(y_train), len(y_test)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
train_y = le.fit_transform(train_label.values)
test_y = le.fit_transform(test_label.values)
train_y, test_y

train_y[:100]

#final tokenization
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

embed_size = 100
max_feature = 50000
max_len = 2000

# Initialize and fit the tokenizer
tokenizer = Tokenizer(num_words=max_feature)
tokenizer.fit_on_texts(x_train)

# Convert texts to sequences
x_train_seq = tokenizer.texts_to_sequences(x_train)
x_test_seq = tokenizer.texts_to_sequences(x_test)


# Pad sequences to ensure length
x_train_padded = pad_sequences(x_train_seq, maxlen=max_len)
x_test_padded = pad_sequences(x_test_seq, maxlen=max_len)


x_train_padded[10], len(x_train_padded[10])

import pickle
with open('tokenizer_123.pkl', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dropout

# Build the model
embedding_size=32
model=Sequential()
model.add(Embedding(input_dim=max_feature, output_dim=embedding_size, input_length=max_len))
model.add(Bidirectional(LSTM(64)))
model.add(Dense(16, activation='relu'))
model.add(Dropout(0.1))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Print the model summary
model.summary()

# Model training
history = model.fit(x_train_padded, train_y, batch_size=64, epochs=3, validation_data=(x_test_padded, test_y))

import sklearn
from sklearn.metrics import confusion_matrix

y_predict=[1 if i>0.5 else 0 for i in model.predict(x_test_padded)]
cf=confusion_matrix(test_y,y_predict)

print(cf)

ax=plt.subplot()
sns.heatmap(cf,annot=True,ax=ax,cmap="Blues",fmt='')

ax.set_xlabel("predicted labels")
ax.set_ylabel("true labels")
ax.set_title("confusion matrix")
ax.xaxis.set_ticklabels(["not spam","spam"])
ax.yaxis.set_ticklabels(["not spam","spam"])

from sklearn.metrics import precision_score, recall_score, f1_score

print("Precision: {:.2f}%".format(100 * precision_score(test_y, y_predict)))
print("Recall: {:.2f}%".format(100 * recall_score(test_y, y_predict)))
print("F1 Score: {:.2f}%".format(100 * f1_score(test_y,y_predict)))























